import os
import torch
from transformers import AutoTokenizer, AutoModel
import numpy as np
from normalize_query import normalise_query, TERMINS
import warnings

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –æ –Ω–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ—Å–∞—Ö
warnings.filterwarnings("ignore", message="Some weights of.*were not initialized")

# ‚úÖ –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
if device.type == "cuda":
    print(f"GPU: {torch.cuda.get_device_name(0)}")

# ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ (–±–µ–∑ 'root/' –≤ –Ω–∞—á–∞–ª–µ)
model_path = "/root/ai_models/ru-en-RoSBERTa"

# ‚úÖ –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ –º–æ–¥–µ–ª—å —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏
tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)
model = AutoModel.from_pretrained(model_path, local_files_only=True)

# ‚úÖ –ü–µ—Ä–µ–≤–æ–¥–∏–º –º–æ–¥–µ–ª—å –Ω–∞ GPU, –≤–∫–ª—é—á–∞–µ–º half precision –∏ eval
if device.type == "cuda":
    model = model.to(device).half()
else:
    model = model.to(device)
model.eval()

# üîπ –§—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
def get_embeddings(texts, remove_prefix=True):
    processed_texts = []
    for text in texts:
        if remove_prefix and text.startswith("search_query:"):
            text = text[len("search_query:"):].strip()
        processed_texts.append(text)

    inputs = tokenizer(
        processed_texts,
        padding=True,
        truncation=True,
        return_tensors="pt",
        max_length=512
    ).to(device)

    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state[:, 0, :]  # [CLS] —Ç–æ–∫–µ–Ω
        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)

    return embeddings.cpu().numpy()

# üîπ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤
def process_queries(queries, termins=TERMINS, batch_size=8):
    results = []
    normalized_queries = [normalise_query(q, termins) for q in queries]

    for i in range(0, len(normalized_queries), batch_size):
        batch = normalized_queries[i:i+batch_size]
        batch_embeddings = get_embeddings(batch)

        for query, norm_q, emb in zip(queries[i:i+batch_size], batch, batch_embeddings):
            results.append({
                "original_query": query,
                "normalized_query": norm_q,
                "embedding": emb,
                "embedding_shape": emb.shape,
                "embedding_norm": np.linalg.norm(emb)
            })
    return results

# üîπ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    test_queries = [
        "–ö–∞–∫ –æ—Ñ–æ—Ä–º–∏—Ç—å –≠–î–û –¥–ª—è 44 —Ñ–∑ –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –õ–ö –æ–ø–µ—Ä–∞—Ç–æ—Ä–∞?",
        "–ö–∞–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –ø–æ–¥–ø–∏—Å–∏ –ø–æ 223-–§–ó?"
    ]

    results = process_queries(test_queries)

    for i, result in enumerate(results):
        print(f"\n--- –ó–∞–ø—Ä–æ—Å {i+1} ---")
        print(f"–û—Ä–∏–≥–∏–Ω–∞–ª: {result['original_query']}")
        print(f"–ù–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–π: {result['normalized_query']}")
        print(f"–†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {result['embedding_shape']}")
        print(f"–ù–æ—Ä–º–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞: {result['embedding_norm']:.4f}")

    # üîπ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ö–æ–∂–µ—Å—Ç–∏
    from sklearn.metrics.pairwise import cosine_similarity

    print("\n--- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ ---")
    for i in range(len(results)):
        for j in range(i + 1, len(results)):
            sim = cosine_similarity(
                [results[i]['embedding']],
                [results[j]['embedding']]
            )[0][0]
            print(f"–°—Ö–æ–∂–µ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞ {i+1} –∏ {j+1}: {sim:.4f}")
